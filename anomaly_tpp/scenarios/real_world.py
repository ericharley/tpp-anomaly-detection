import numpy as np
import torch

from anomaly_tpp.data import SequenceDataset
import pathlib


file_path = pathlib.Path(__file__).parent.resolve()


class STEAD:
    """Timestamps of earthquakes in different geographic regions.

    References:
        Mousavi, S. M., Sheng, Y., Zhu, W., Beroza G.C., (2019). STanford EArthquake
        Dataset (STEAD): A Global Data Set of Seismic Signals for AI, IEEE Access,
        doi:10.1109/ACCESS.2019.2947848

    Attributes:
        id_train: 4000 sequences from San Mateo, CA used for training.
        id_test: 1000 sequences from San Mateo, CA used for testing.
        ood_test_datasets: list of OOD (out-of-distribution) datasets
            ood_test_datasets[0]: 1000 OOD sequences for Anchorage, AK
            ood_test_datasets[1]: 1000 sequences for Aleutian Islands, AK
            ood_test_datasets[2]: 1000 sequences for Helmet, CA

    """
    def __init__(self, num_train=4000, seed=123):
        np.random.seed(seed)
        data_dir = file_path / "../../data/stead/"

        if num_train > 5000 or num_train < 1:
            raise ValueError(f"num_train must be between 1 and 4999")

        # Dataset containing 5000 the in-distribution sequences
        # We shuffle the sequences, and take first num_train for training
        # Remaining (5000 - num_train) will be used for testing
        id_full_dataset = SequenceDataset.from_file(data_dir / "centroid_1.pt")
        indices = np.random.permutation(len(id_full_dataset)).astype(int)
        self.id_train = SequenceDataset([id_full_dataset[i] for i in indices[:num_train]])
        self.id_test = SequenceDataset([id_full_dataset[i] for i in indices[num_train:]])

        self.ood_test_datasets = {}
        ood_region_names = ["Anchorage, AK", "Aleutian Islands, AK", "Helmet, CA"]
        for idx, name in zip([2, 3, 4], ood_region_names):
            ood_full_dataset = SequenceDataset.from_file(data_dir / f"centroid_{idx}.pt")
            ood_test_sequences = ood_full_dataset.sequences[num_train:]
            self.ood_test_datasets[name] = SequenceDataset(ood_test_sequences)


class ServerLogs:
    """Server logs generated by Sockshop Microservices Docker application.

    In-distribution sequences correspond to normal operations, out-of-distribution
    sequences correspond to periods of operation when some corruption was inserted
    using Pumba chaos testing tool

    References:
        Sockshop: https://github.com/microservices-demo/microservices-demo
        Pumba: https://github.com/alexei-led/pumba
    """
    def __init__(self):
        data_dir = file_path / "../../data/logs/"
        self.id_train = SequenceDataset.from_file(data_dir / "normal_30sec.pt")
        d_test = SequenceDataset.from_file(data_dir / "attacks_30sec.pt")
        self.id_test = SequenceDataset([seq for seq in d_test if seq["metadata"]["label"] == "normal"])

        ood_scenario_names = [
            "Packet delay (frontend)",
            "Packet corruption (10%)",
            "Packet corruption (1%)",
            "Packet delay (all services)",
            "Packet duplication(1%)",
        ]

        self.ood_test_datasets = {}
        for idx, name in zip([1, 2, 3, 4, 5], ood_scenario_names):
            sequences = [seq for seq in d_test if seq["metadata"]["label"] == idx]
            self.ood_test_datasets[name] = SequenceDataset(sequences)
